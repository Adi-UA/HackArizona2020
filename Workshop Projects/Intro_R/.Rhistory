install.packages("tidyverse")
library(tidyverse)
?install.packages
head(iris)
summary(iris)
R.version
# Load the tidyverse package
library(tidyverse)
iris_means
# Calculate mean value
iris_means <- summarise(iris_grouped, SL_mean = mean(Sepal.length))
# Group iris data by values in species column
iris_grouped <- group_by(iris, Species)
# Calculate mean value
iris_means <- summarise(iris_grouped, SL_mean = mean(Sepal.length))
# Calculate mean value
iris_means <- summarise(iris_grouped, SL_mean = mean(Sepal.Length))
iris_means
# Group data, calculate means
iris_means <- iris %>%
group_by(Species) %>%
summarise(SL_mean = mean(Sepal.Length),
SL_se = sd(Sepal.Length)/sqrt(n()))
iris_means
# Convert to long
# -Species excludes the species category
iris_long <- gather(iris, key = trait, value = measurement, -Species)
head(iris_long)
# Group data, calculate means
iris_means <- iris %>%
gather(key = trait, value = measurement, -Species) %>%
group_by(Species) %>%
summarise(trait_mean = mean(measurement),
trait_se = sd(measurement)/sqrt(n()))
# Group data, calculate means
iris_means <- iris %>%
gather(key = trait, value = measurement, -Species)
iris_means
# Group data, calculate means
iris_means <- iris %>%
gather(key = trait, value = measurement, -Species) %>%
group_by(Species, trait) %>%
summarise(trait_mean = mean(measurement),
trait_se = sd(measurement)/sqrt(n()))
iris_means
# Group data, calculate means
iris_means <- iris %>%
gather(key = trait, value = measurement, -Species) %>%
group_by(Species, trait) %>%
summarise(trait_mean = mean(measurement),
trait_se = sd(measurement)/sqrt(n()))
# Plot iris data
ggplot(data = iris_means, mapping = aes( x= Species, y =  trait_mean)) +
geom_point()+geom_errorbar(mapping = aes(ymin = trait_mean -trait_se,
ymax = trait_mean + trait_se,
width = 0.3))
# Plot iris data
ggplot(data = iris_means, mapping = aes( x= Species, y =  trait_mean)) +
geom_point()+geom_errorbar(mapping = aes(ymin = trait_mean -trait_se,
ymax = trait_mean + trait_se,
width = 0.3))
facet_wrap(~trait, scales = "free_y")
# Plot iris data
ggplot(data = iris_means, mapping = aes( x= Species, y =  trait_mean)) +
geom_point()+geom_errorbar(mapping = aes(ymin = trait_mean -trait_se,
ymax = trait_mean + trait_se,
width = 0.3))
facet_wrap(~trait, scales = "free_y")
# Plot iris data
ggplot(data = iris_means, mapping = aes( x= Species, y =  trait_mean)) +
geom_point()+geom_errorbar(mapping = aes(ymin = trait_mean -trait_se,
ymax = trait_mean + trait_se,
width = 0.3)) +
facet_wrap(~trait, scales = "free_y")
ggsave(filename = "iris-plot.pdf")
install.packages("rtweet")
install.packages("tidytext")
install.packages("readr")
install.packages("readr")
library(rtweet)
from twitter every 9 minutes
# if it reaches twitters limit of data
# retryonratelimit times its requests from twitter every 9 minutes
iran_tweets <- search_tweets("iran",
n = 5000,
lang = "en",
iinclude_rts = FALSE,
retryonratelimit = TRUE)
iran_tweets <- search_tweets("iran",
n = 5000,
lang = "en",
iinclude_rts = FALSE,
retryonratelimit = TRUE)
iran_tweets <- search_tweets("iran",
n = 5000,
lang = "en",
iinclude_rts = FALSE,
retryonratelimit = TRUE)
dir("data)
# retryonratelimit times its requests from twitter every 9 minutes
# if it reaches twitters limit of data
dir("data")
dir("data)
# retryonratelimit times its requests from twitter every 9 minutes
# if it reaches twitters limit of data
install.packages("readr")
setwd("D:/Common/Projects/R Projects/Twitter_Mining/data")
iran_tweets <- read_csv(iran_tweets)
iran_tweets <- read_tsv(iran_tweets)
iran_tweets <- read_tsv("iran_tweets.tsv")
iran_tweets <- read_tsv("data/iran_tweets.tsv")
library(readr)
iran_tweets <- read_tsv("data/iran_tweets.tsv")
iran_tweets <- read_tsv("iran_tweets.tsv")
nrow(iran_tweets)
library(tidyverse)
library(tidytext)
iran_tweets$text # accessses a column
# tokenize tweets. SPlits strings by delimiter.
tokenize_tweets <- iran_tweets %>%
unnest_tokens(word, text)
head(tokenize_tweets)
View(tokenize_tweets)
View(tokenize_tweets)
View(tokenize_tweets)
View(tokenize_tweets)
word_count <- tokenize_tweets %>%
count(word,sort=TRUE)
View(word_count)
View(word_count)
get_stopwords()
# clean the stopwords
clean_tokeized_tweets <- tokenize_tweets %>%
anti_join(get_stopwords())
View(clean_tokeized_tweets)
View(clean_tokeized_tweets)
# clean word count
clean_word_count <- clean_tokeized_tweets %>%
count(word,sort = TRUE)
View(clean_word_count)
View(clean_word_count)
# clean word count
clean_word_count <- clean_tokeized_tweets %>%
dplyr::count(word,sort = TRUE)
View(clean_word_count)
View(clean_word_count)
View(clean_word_count)
View(clean_word_count)
iran_tweets_ny <- read_tsv("iran_tweets_ny")
iran_tweets_az <- read_tsv("iran_tweets_az")
iran_tweets_az <- read_tsv("iran_tweets_az.tsv")
iran_tweets_ny <- read_tsv("iran_tweets_ny.tsv")
tokenize_tweets_ny = iran_tweets_ny %>%
unnest_tokens(word,text)
tokenize_tweets_az = iran_tweets_az %>%
unnest_tokens(word,text)
clean_tokeized_tweets_ny <- tokenize_tweets_ny %>%
anti_join(get_stopwords())
clean_tokeized_tweets_az <- tokenize_tweets_az %>%
anti_join(get_stopwords())
clean_word_count_ny <- clean_tokeized_tweets_ny %>%
dplyr::count(word,sort = TRUE)
clean_word_count_az <- clean_tokeized_tweets_az %>%
dplyr::count(word,sort = TRUE)
View(clean_word_count_az)
View(clean_word_count_az)
View(clean_word_count_ny)
View(clean_word_count_ny)
view(clean_word_count_az)
view(clean_word_count_ny)
View(clean_word_count_az)
View(clean_word_count_az)
View(clean_word_count_ny)
View(clean_word_count_ny)
iran_tweets_az$my_location <- "arizona"
iran_tweets_ny$my_location <- "new york"
combined_tweeets <- bind_rows(iran_tweets_az, iran_tweets_ny)
unique(combined_tweeets$my_location)
combined_tweets <- bind_rows(iran_tweets_az, iran_tweets_ny)
unique(combined_tweeets$my_location)
View(combined_tweeets)
View(combined_tweeets)
# tokenize
combined_tweets_tokens <- combined_tweets %>%
unnest_tokens(word,text)
# creating a comparable dataframe
word_count_combined <- combined_tweets_tokens %>%
group_by(my_location,word) %>%
summmarise(n = n()) %>% # n() is the same as count used earleir
mutate(total = sum(n),
norm_freq = n/total)
# creating a comparable dataframe
word_count_combined <- combined_tweets_tokens %>%
group_by(my_location,word) %>%
summarise(n = n()) %>% # n() is the same as count used earleir
mutate(total = sum(n),
norm_freq = n/total)
View(word_count_combined)
View(word_count_combined)
# for the idf frequency calculation
word_count_combined_v2 = word_count_combined %>%
bind_tf_idf(word,my_location,n)
View(word_count_combined)
View(word_count_combined)
View(word_count_combined)
View(word_count_combined)
View(word_count_combined_v2)
View(word_count_combined_v2)
